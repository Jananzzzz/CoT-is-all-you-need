{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LTP\n",
    "Link: \n",
    "https://github.com/HIT-SCIR/ltp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltp import LTP\n",
    "ltp = LTP() # default LTP/small model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['汤姆生病了。', '他去了医院。']\n",
      "['汤姆生病了。', '他去了医院。', '汤姆生病了。', '他去了医院。']\n"
     ]
    }
   ],
   "source": [
    "from ltp import StnSplit\n",
    "sents1 = StnSplit().split(\"汤姆生病了。他去了医院。\")\n",
    "sents2 = StnSplit().batch_split([\"汤姆生病了。他去了医院。\", \"汤姆生病了。他去了医院。\"])\n",
    "print(sents1)\n",
    "print(sents2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-defined dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltp.add_words(\"长江大桥\", freq=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['他', '叫', '汤姆', '去', '拿', '外衣', '。']]\n",
      "[['r', 'v', 'nh', 'v', 'v', 'n', 'wp']]\n",
      "[[('Nh', '汤姆')]]\n",
      "[[{'predicate': '叫', 'arguments': [('A0', '他'), ('A1', '汤姆'), ('A2', '去拿外衣')]}, {'predicate': '拿', 'arguments': [('A0', '汤姆'), ('A1', '外衣')]}]]\n",
      "[{'head': [2, 0, 2, 5, 2, 5, 2], 'label': ['SBV', 'HED', 'DBL', 'ADV', 'VOB', 'VOB', 'WP']}]\n",
      "[{'head': [2, 0, 2, 2, 4, 5, 2], 'label': ['AGT', 'Root', 'DATV', 'eSUCC', 'eSUCC', 'PAT', 'mPUNC']}]\n",
      "[[(1, 2, 'AGT'), (2, 0, 'Root'), (3, 2, 'DATV'), (3, 4, 'AGT'), (3, 5, 'AGT'), (4, 2, 'eSUCC'), (5, 2, 'eSUCC'), (5, 4, 'eSUCC'), (6, 5, 'PAT'), (7, 2, 'mPUNC')]]\n"
     ]
    }
   ],
   "source": [
    "result = ltp.pipeline([\"他叫汤姆去拿外衣。\"], tasks=['cws', 'pos', 'ner', 'srl', 'dep', 'sdp', 'sdpg'])\n",
    "# chinese word segmentation\n",
    "print(result.cws)\n",
    "\n",
    "# part-of-speech tagging\n",
    "print(result.pos) # \"r\" stands for pronoun, \"v\" stands for verb, \"nh\" stands for person name, \"wp\" stands for punctuation\n",
    "\n",
    "# named entity recognition\n",
    "print(result.ner)\n",
    "\n",
    "# semantic role labeling\n",
    "print(result.srl)\n",
    "\n",
    "# dependency parsing\n",
    "print(result.dep)\n",
    "\n",
    "# semantic dependency parsing\n",
    "print(result.sdp)\n",
    "\n",
    "# semantic dependency parsing with graph\n",
    "print(result.sdpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
